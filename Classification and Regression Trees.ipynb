{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. First things first: What is machine learning? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Machine learning is a field of computer science that uses statistical techniques to give computer systems the ability to \"learn\" (i.e., progressively improve performance on a specific task) with data, without being explicitly programmed.\" (https://en.wikipedia.org/wiki/Machine_learning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](AI_ML_DL.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AI can sense, reason, (re)act and adapt.\n",
    "\n",
    "ML is a collection of self-learning algorithms. \n",
    "\n",
    "DL is a subset of ML. neural networks with several layers can learn from huge amounts of data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Types of machine learning algorithms "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Supervised learning\n",
    "\n",
    "The data is labeled and the algorithms at hand is trained to predict the output from the input data.\n",
    "\n",
    "**Examples**\n",
    "- Decision Trees (classification and regression trees)\n",
    "- Support Vector Machines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Unsupervised learning\n",
    "\n",
    "\n",
    "The data is unlabeled and the algorithms at hand learns from the structure of the input data to gain insights and to detect certain patterns\n",
    "\n",
    "**Examples**\n",
    "- Principal Component Analysis\n",
    "- k-means Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"DataScience_Continuum.png\" alt=\"Drawing\" style=\"width: 800px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(source: http://blog.applied.ai/bayesian-inference-with-pymc3-part-1/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Decision Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To explain what a tree is and how to construct it, let's have a look at an introductory one first. This tree is constructred to answer the question whether or not someone has watched the HBO series \"Game of Thrones\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](G_of_T_tree.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why?: interpretability.\n",
    "\n",
    "Terminology: root, leave nodes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 How to construct a tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.1 Gini and tree splitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Questions:\n",
    "- What should be the cutoff age? \n",
    "- if multiclass problems, how to split them up?\n",
    "\n",
    "Impurity measure: minimize the impurity in each node!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In each node n, compute the gini impurity for a set of items with $J$ classes (J referring to how many splits (2-way, 3-way,...):\n",
    "\n",
    "$gini_n$ = $\\displaystyle\\sum_{i=1}^J p_i(1-p_i) = 1- \\displaystyle\\sum^J_{i=1}p_i^2$\n",
    "\n",
    "The gini measure takes values between 0 and 0.5, 0 being a \"perfectly pure node\" and 0.5 being a 50/50 split.\n",
    "\n",
    "What you'll want to do is create a split that maximized the purity gain, which is done through computing weighted gini impurities of the two nodes that are the result of a split. Denote $gini_n1$ and $gini_n2$ the gini impurities of two nodes that are the result of a split from node $n0$. The resulting gain can me computed as:\n",
    "\n",
    "$gain$ = $gini_{n0} - prop_{n1} * gini_{n1} -prop_{n2}* gini_{n2} $. \n",
    "\n",
    "Where $prop_{n1}$ and $prop_{n2}$ denote the proportions of cases in each node compared to the parent node.\n",
    "\n",
    "The goal is to maximize the gain. In practice, this is sometimes computed by minimizing:\n",
    "\n",
    "$prop_{n1} * gini_{n1} + prop_{n2}* gini_{n2} $. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem of learning an optimal decision tree is a very hard problem. Consequently, practical decision-tree learning algorithms are based on a heuristic algorithm known as the **greedy algorithm**. This algorithm makes locally optimal decisions at each node, starting from the root node. Algorithms like the greedy algorithm cannot guarantee that the resulting tree is globally optimal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.2  Entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Pruning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "about **pruning**:\n",
    "\n",
    "There are two common strategies to prevent overfitting: stopping the creation of the tree early (also called pre-pruning), or building the tree but then removing or collaps‚Äê ing nodes that contain little information (also called post-pruning or just pruning). Possible criteria for pre-pruning include limiting the maximum depth of the tree, limiting the maximum number of leaves, or requiring a minimum number of points in a node to keep splitting it. Decision trees in scikit-learn are implemented in the DecisionTreeRegressor and DecisionTreeClassifier classes. scikit-learn only implements pre-pruning, not post-pruning. (Muller, Guido \"ML in Python)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.svds.com/machine-learning-vs-statistics/\n",
    "https://github.com/xbno/Projects/blob/master/Models_Scratch/Decision%20Trees%20from%20scratch.ipynb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
